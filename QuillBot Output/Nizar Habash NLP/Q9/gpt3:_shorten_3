I would utilize transfer learning to develop an Arabic-to-Finnish machine translation system with minimal parallel data and a short timescale. Using a little amount of parallel data from the Europarl and UN corpus, I could fine-tune a pre-trained model like the Moses MT system or one of the transformer models (e.g. T5, BERT). Preprocessing and cleaning the data, then using parallel data to fine-tune the pre-trained model. After that, I would utilize the leftover cash to collect parallel data from internet forums and social media sites to refine the model. To test the model's performance, I would use BLEU, METEOR, and ROUGE metrics to quantify translation quality. Native speakers of both languages would also review the translations. Finally, I would use the model to translate a series of phrases and assess its ability to maintain meaning to ensure that the translations are both grammatically accurate and meaningful. With such a short timeframe and minimal resources, excellent performance will be difficult. In this case, transfer learning and fine-tuning using pre-trained models may develop a machine translation system. 