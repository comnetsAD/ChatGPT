To create an Arabic-to-Finnish machine translation system with minimal parallel data and in a short amount of time, I would likely use transfer learning. One approach would be to fine-tune a pre-trained model, such as the Moses MT system or one of the transformer models (e.g. T5, BERT), utilizing a limited amount of parallel data from the Europarl and UN corpora. I would begin by preprocessing and cleansing the data, and then I would utilize parallel data to fine-tune the pre-trained model. The leftover money would then be used to collect more parallel data from other sources, such as internet forums or social media platforms, in order to enhance the performance of the model. To test the success of the model, I would utilize a range of translation quality indicators, such as BLEU, METEOR, and ROUGE. I would also do human review by having the translations evaluated by fluent speakers of both languages. To ensure that the translations are not only grammatically accurate but also comprehensible, I would use the model to translate a collection of sentences and assess the model's ability to maintain the meaning of the phrases. Noting that reaching good performance with such a short timeline and minimal resources would be difficult is essential. In this situation, however, transfer learning and fine-tuning using pre-trained models may still be a useful method for developing a machine translation system. 