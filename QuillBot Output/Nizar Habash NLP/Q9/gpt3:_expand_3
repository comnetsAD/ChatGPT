Transfer learning is the strategy that I would most likely employ if I were to develop a machine translation system from Arabic to Finnish in a short amount of time and with a limited amount of parallel data. Using a little amount of parallel data that I can get from the Europarl and UN corpus would be one method for me to fine-tune a pre-trained model such as the Moses MT system or one of the transformer models (such as T5, BERT). This would be one approach to go about doing this. First, I would preprocess the data and clean it up, and then I would use the parallel data to fine-tune the model that had already been pre-trained. After that, I would utilize the money that was left over in the budget to collect more parallel data from other sources, such as online forums or social media platforms, in order to enhance the functionality of the model. I would utilize a number of metrics to quantify the quality of the translations in order to evaluate the success of the model. Some examples of these metrics are BLEU, METEOR, and ROUGE. I would also undertake human review by having native speakers of both languages analyze the translations. This would be an additional step in the evaluation process. To conclude, I would also use the model to translate a collection of phrases and assess the model's ability to maintain the meaning of the sentences. This would be done in order to guarantee that the translations are not only grammatically accurate but also make sense. It is essential to emphasize the fact that reaching good performance with such a short amount of time and a restricted amount of resources would be difficult. In spite of this, constructing a machine translation system by using transfer learning and fine-tuning with pre-trained models might still be a successful strategy. 