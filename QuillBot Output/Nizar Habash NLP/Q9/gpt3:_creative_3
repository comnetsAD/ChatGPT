If I had to create an Arabic to Finnish machine translation system with little time and data to work with in parallel, I would probably employ transfer learning. To achieve this, I may use the Europarl and UN corpus to get a tiny amount of parallel data and then use that data to fine-tune a pre-trained model, such the Moses MT system or one of the transformer models (e.g. T5, BERT). To begin, I would clean and prepare the data for analysis, and then I would utilize the parallel data to adjust the parameters of the already-trained model. The leftover funds would then be used toward acquiring more parallel data from other sources, such as internet forums or social media platforms, with the goal of enhancing the model's performance. I would utilize many measures, including BLEU, METEOR, and ROUGE, to assess the model's efficacy in terms of translation quality. If I were to do human review, I'd have bilingual individuals assess the accuracy of the translations. Finally, to ensure the translations are both grammatically accurate and make sense, I would use the model to translate a series of phrases and assess the model's ability to maintain the meaning of the words. It's worth noting that due to the short timeline and limited resources, obtaining good performance will be difficult. Building a machine translation system may still benefit from transfer learning and fine-tuning using pre-trained models. 